{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "https://github.com/muneer-kayali/autodubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1.1-essentials_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with gcc 14.2.0 (Rev1, Built by MSYS2 project)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-zlib --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-sdl2 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libgme --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libtheora --enable-libvo-amrwbenc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-librubberband\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.101 / 61. 19.101\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "[in#0 @ 0000025303fd5200] Error opening input: No such file or directory\n",
      "Error opening input file video_input.mp4.\n",
      "Error opening input files: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -y -i video_input.mp4 -q:a 0 audio_input.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_whisper\n",
    "model = stable_whisper.load_model('large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\anaconda3\\envs\\AutoDubbing\\Lib\\site-packages\\stable_whisper\\whisper_word_level\\original_whisper.py:249: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Transcribe:   0%|          | 0/98.01 [01:12<?, ?sec/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribe: 100%|██████████| 98.01/98.01 [08:53<00:00,  5.44s/sec]\n",
      "Transcribe:   0%|          | 0/81.15 [01:02<?, ?sec/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribe: 100%|██████████| 81.15/81.15 [02:51<00:00,  2.12s/sec]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "video = \"15\"\n",
    "video2 = \"16\"\n",
    "audio = os.path.join(\"data\", \"processed\", f\"video_{video}\", \"extracted_audio.wav\")\n",
    "audio2 = os.path.join(\"data\", \"processed\", f\"video_{video2}\", \"extracted_audio.wav\")\n",
    "result = model.transcribe(audio, vad=True, min_word_dur=0.3, vad_threshold=0.45, nonspeech_error=0.25)\n",
    "result2 = model.transcribe(audio2, vad=True, min_word_dur=0.3, vad_threshold=0.45, nonspeech_error=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_segments = []\n",
    "for segment in result2.segments:\n",
    "    all_segments.append({\"start\": segment.start, \"end\": segment.end, \"text\": segment.text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 1: ' Oh, hi.', Start: '0.2, End: '0.78\n",
      "Segment 2: ' You know what?', Start: '0.96, End: '1.84\n",
      "Segment 3: ' Um,', Start: '2.08, End: '2.22\n",
      "Segment 4: ' before I get in the box,', Start: '2.24, End: '3.8\n",
      "Segment 5: ' could I just run to the restroom', Start: '3.94, End: '5.0\n",
      "Segment 6: ' and make sure my hair is perfect?', Start: '5.0, End: '6.22\n",
      "Segment 7: ' Fine.', Start: '6.68, End: '6.92\n",
      "Segment 8: ' Can you be speedy about it?', Start: '7.16, End: '8.12\n",
      "Segment 9: ' Mm-hmm.', Start: '8.2, End: '8.42\n",
      "Segment 10: ' Down this way?', Start: '10.16, End: '10.86\n",
      "Segment 11: ' It's just down the hallway.', Start: '10.96, End: '11.88\n",
      "Segment 12: ' Thank you.', Start: '12.3, End: '12.72\n",
      "Segment 13: ' On the right.', Start: '13.2, End: '13.62\n",
      "Segment 14: ' Guess you really had to go to the bathroom.', Start: '18.1, End: '19.68\n",
      "Segment 15: ' Get that Barbie!', Start: '20.74, End: '22.176\n",
      "Segment 16: ' Don't you dare push that button.', Start: '25.02, End: '26.18\n",
      "Segment 17: ' Let me push it.', Start: '26.24, End: '26.76\n"
     ]
    }
   ],
   "source": [
    "for i, segment in enumerate(all_segments):\n",
    "    print(f\"Segment {i + 1}: '{segment['text']}', Start: '{segment['start']}, End: '{segment['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variable OPENAI_API_KEY set successfully:\n",
      "\n",
      "SUCCESS: Specified value was saved.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "api_key = \"ENTER YOUR OPENAI API KEY HERE\"\n",
    "\n",
    "# Set the environment variable using PowerShell\n",
    "command = [\"powershell\", \"-Command\", f\"setx OPENAI_API_KEY \\\"{api_key}\\\"\"]\n",
    "result1 = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Check if the command succeeded\n",
    "if result1.returncode == 0:\n",
    "    print(f\"Environment variable OPENAI_API_KEY set successfully:\\n{result1.stdout}\")\n",
    "else:\n",
    "    print(f\"Failed to set environment variable:\\n{result1.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response:\n",
      " Speaker: Speaker 1, Text: Let's take a look at the weather forecast now., Start: 0.192, End: 2.62\n",
      "Speaker: Speaker 1, Text: I'm delighted to say we've got a new member of our weather team tonight. Let me hand over to him now., Start: 2.82, End: 8.096\n",
      "Speaker: Speaker 2, Text: Well, it's an unsettled picture as we head towards the end of the week., Start: 8.224, End: 13.888\n",
      "Speaker: Speaker 2, Text: This afternoon it'll be cold, wet and windy across most of Scotland. We're under the influence of low pressure and this weather front pushing northwards is bringing cloud and outbreaks of rain., Start: 14.592, End: 27.808\n",
      "Speaker: Speaker 2, Text: The rain, of course, will be heaviest over the borders and around Edinburgh, where it could lead to difficult conditions on the roads., Start: 27.968, End: 35.584\n",
      "Speaker: Speaker 2, Text: In the west, rain will be lighter and patchier. There will maybe a few drier interludes over Dumfries House in Ayrshire., Start: 36.384, End: 43.264\n",
      "Speaker: Speaker 2, Text: Aha! There'll be snow for the higher ground of the Highlands and Aberdeenshire. The potential for a few flurries over Balmoral., Start: 43.616, End: 51.36\n",
      "Speaker: Speaker 2, Text: Who the hell wrote this script?, Start: 51.392, End: 52.38\n",
      "Speaker: Speaker 2, Text: As the afternoon goes on. The best of the drier and brighter weather will, of course, be over the Northern Isles and the far north of the mainland., Start: 52.74, End: 61.28\n",
      "Speaker: Speaker 2, Text: So, a little hazy sunshine to the Castle of May and Caithness, but a cold day everywhere with temperatures of just 8 Celsius and a brisk north-easterly wind., Start: 61.66, End: 70.784\n",
      "Speaker: Speaker 2, Text: Thank God it isn't a bank holiday., Start: 71.36, End: 72.5\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "def group_and_merge_segments_by_speaker(segments):\n",
    "    \"\"\"\n",
    "    Groups segments by speaker and merges adjacent segments spoken by the same speaker.\n",
    "\n",
    "    Args:\n",
    "        segments (list): List of segments, each with 'text', 'start', and 'end' keys.\n",
    "\n",
    "    Returns:\n",
    "        list: Merged segments with speaker information.\n",
    "    \"\"\"\n",
    "    merged_segments = []  # List to store the final merged segments\n",
    "\n",
    "    # Construct the prompt with all segments\n",
    "    prompt_intro = (\n",
    "        \"You are an expert at analyzing conversational text to determine speakers in a dialogue. \"\n",
    "        \"The text segments provided are from a conversation involving two speakers or more speakers: 'Speaker 1', 'Speaker 2', ..., 'Speaker n'. \"\n",
    "        \"Your task is to classify each segment into one of these speakers based on their style, content, or clues. \"\n",
    "        \"If a segment clearly belongs to one speaker, respond with 'Speaker i' for i in all speakers. \"\n",
    "        \"If the speaker cannot be confidently determined, force yourself to make a decision. Furthermore, you should group the segments if two or more adjacent segments are spoken by the same speaker, however, this should only be done if the sentences are spoken right away (if there is silence of 2 seconds or more then dont group), Also, the segments should not be long, so if regrouping gives you a long segment of multiple sentences (2-3), then this should not be done. Here are the segments:\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Append all segments to the prompt\n",
    "    for i, segment in enumerate(segments):\n",
    "        prompt_intro += f\"Segment {i + 1}: '{segment['text']}', Start: '{segment['start']}, End: '{segment['end']}\\n\"\n",
    "\n",
    "    prompt_intro += (\n",
    "        \"\\nPlease provide your output after classifying and regrouping in the following format:\\n\"\n",
    "        \"Speaker: speaker_num, Text: text, Start: start, End: end\\n\"\n",
    "        \"Speaker: speaker_num, Text: text, Start: start, End: end\\n\"\n",
    "        \"Speaker: speaker_num, Text: text, Start: start, End: end\\n\"\n",
    "        \"Avoid formatting the text with bold or italics. Use the exact format as shown above.\\n\"\n",
    "    )\n",
    "\n",
    "    # Call the OpenAI API with the constructed prompt\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_intro}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract the model's response\n",
    "    response = completion.choices[0].message.content.strip()\n",
    "    print(\"API Response:\\n\", response)\n",
    "    \n",
    "    regrouped_segments = []\n",
    "\n",
    "    # Split response into lines\n",
    "    response_lines = response.splitlines()\n",
    "\n",
    "    for line in response_lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines or irrelevant content\n",
    "        if not line or not line.startswith(\"Speaker:\"):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Parse the line into components\n",
    "            speaker = line.split(\"Speaker:\")[1].split(\", Text:\")[0].strip()\n",
    "            text = line.split(\"Text:\")[1].split(\", Start:\")[0].strip()\n",
    "            start = float(line.split(\"Start:\")[1].split(\", End:\")[0].strip())\n",
    "            end = float(line.split(\"End:\")[1].strip())\n",
    "\n",
    "            # Add parsed segment to the list\n",
    "            regrouped_segments.append({\n",
    "                \"speaker\": speaker,\n",
    "                \"text\": text,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            })\n",
    "        except (IndexError, ValueError) as e:\n",
    "            print(f\"Error parsing line: {line}, Error: {e}\")\n",
    "\n",
    "    return regrouped_segments\n",
    "\n",
    "regrouped_segments = group_and_merge_segments_by_speaker(all_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: tempfiles\\segment0.wav\n",
      "Saved: tempfiles\\segment1.wav\n",
      "Saved: tempfiles\\segment2.wav\n",
      "Saved: tempfiles\\segment3.wav\n",
      "Saved: tempfiles\\segment4.wav\n",
      "Saved: tempfiles\\segment5.wav\n",
      "Saved: tempfiles\\segment6.wav\n",
      "Saved: tempfiles\\segment7.wav\n",
      "Saved: tempfiles\\segment8.wav\n",
      "Saved: tempfiles\\segment9.wav\n",
      "Saved: tempfiles\\segment10.wav\n"
     ]
    }
   ],
   "source": [
    "from pysrt import SubRipFile\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "audio = AudioSegment.from_mp3(\"audio_input.mp3\")\n",
    "\n",
    "# Function to slice audio based on .srt segments\n",
    "def slice_audio_by_srt(audio, segments):\n",
    "    output_dir = \"tempfiles\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    audio_segments = []  # List to store audio segments\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        # Convert start and end times to milliseconds\n",
    "        start_time = int(segment[\"start\"] * 1000)  # Convert seconds to milliseconds\n",
    "        end_time = int(segment[\"end\"] * 1000)  # Convert seconds to milliseconds\n",
    "\n",
    "        # Slice the audio based on timestamps\n",
    "        audio_slice = audio[start_time:end_time]\n",
    "        audio_segments.append(audio_slice)\n",
    "\n",
    "        # Create a file path for the sliced audio\n",
    "        segment_path = os.path.join(output_dir, f\"segment{i}.wav\")\n",
    "\n",
    "        # Save each segment as a separate file\n",
    "        audio_slice.export(segment_path, format=\"wav\")\n",
    "        print(f\"Saved: {segment_path}\")\n",
    "\n",
    "    return audio_segments\n",
    "\n",
    "sliced_segments = slice_audio_by_srt(audio, regrouped_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "def synthesize_text(input, ssml=False):\n",
    "    \"\"\"Synthesizes speech from the input string of text.\"\"\"\n",
    "    from google.cloud import texttospeech\n",
    "\n",
    "    \n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "    if not ssml:\n",
    "        input_text = texttospeech.SynthesisInput(text=input)\n",
    "    else:\n",
    "        input_text = texttospeech.SynthesisInput(ssml=input)\n",
    "\n",
    "    # Note: the voice can also be specified by name.\n",
    "    # Names of voices can be retrieved with client.list_voices().\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"da-DK\",\n",
    "        name=\"da-DK-Neural2-D\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,\n",
    "    )\n",
    "\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "\n",
    "    response = client.synthesize_speech(\n",
    "        request={\"input\": input_text, \"voice\": voice, \"audio_config\": audio_config}\n",
    "    )\n",
    "\n",
    "    # The response's audio_content is binary.\n",
    "    return response.audio_content\n",
    "    # with open(\"output.mp3\", \"wb\") as out:\n",
    "    #     out.write(response.audio_content)\n",
    "    #     print('Audio content written to file \"output.mp3\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"SET PATH TO GOOGLE CLOUD JSON FILE HERE...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "auth_key = \"ENTER YOUR DEEPL KEY HERE...\"  # Replace with your key\n",
    "translator = deepl.Translator(auth_key)\n",
    "\n",
    "def translate(text):\n",
    "    result = translator.translate_text(text, target_lang=\"DA\")\n",
    "    return result.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_segments = []\n",
    "for segment in regrouped_segments:\n",
    "    translated_text = translate(segment[\"text\"])\n",
    "    translated_segments.append({\n",
    "        \"start\": segment['start'],\n",
    "        \"end\": segment['end'],\n",
    "        \"target_duration\": segment['end'] - segment['start'],\n",
    "        \"text\": translated_text\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_audio_files = []\n",
    "for segment in translated_segments:\n",
    "    translated_audio_files.append(synthesize_text(segment['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "durations = []\n",
    "for segment in translated_audio_files:\n",
    "    audio = AudioSegment.from_file(BytesIO(segment), format=\"mp3\")\n",
    "    duration = len(audio)\n",
    "    durations.append(duration)\n",
    "durations = [d / 1000 for d in durations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, segment in enumerate(translated_segments):\n",
    "    segment['source_duration'] = durations[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, audio_file in enumerate(translated_audio_files):\n",
    "    with open(os.path.join(\"tempfiles\", f\"translated_segment{i}.wav\"), \"wb\") as f:\n",
    "        f.write(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Speaker 1 audio to: speaker_audio\\speaker_1.wav\n",
      "Saved Speaker 2 audio to: speaker_audio\\speaker_2.wav\n"
     ]
    }
   ],
   "source": [
    "def split_audio_by_speaker(grouped_segments, original_audio_path, output_dir):\n",
    "    \"\"\"\n",
    "    Splits audio into separate tracks for each speaker and saves them as separate files.\n",
    "\n",
    "    Args:\n",
    "        grouped_segments (list): List of dictionaries containing speaker, text, start, and end timestamps.\n",
    "        original_audio_path (str): Path to the original audio file.\n",
    "        output_dir (str): Directory to save the split audio files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the original audio\n",
    "    audio = AudioSegment.from_file(original_audio_path)\n",
    "\n",
    "    # Dictionary to store audio for each speaker\n",
    "    speaker_audio = {}\n",
    "\n",
    "    # Iterate through grouped segments and append audio to the respective speaker\n",
    "    for entry in grouped_segments:\n",
    "        speaker = entry[\"speaker\"]\n",
    "        start_time = int(entry[\"start\"] * 1000)  # Convert to milliseconds\n",
    "        end_time = int(entry[\"end\"] * 1000)  # Convert to milliseconds\n",
    "\n",
    "        # Initialize silent audio for the speaker if not already in dictionary\n",
    "        if speaker not in speaker_audio:\n",
    "            speaker_audio[speaker] = AudioSegment.silent(duration=0)\n",
    "\n",
    "        # Slice the audio and append to the corresponding speaker\n",
    "        speaker_audio[speaker] += audio[start_time:end_time]\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the audio files for each speaker\n",
    "    for speaker, speaker_track in speaker_audio.items():\n",
    "        output_path = os.path.join(output_dir, f\"{speaker.replace(' ', '_').lower()}.wav\")\n",
    "        speaker_track.export(output_path, format=\"wav\")\n",
    "        print(f\"Saved {speaker} audio to: {output_path}\")\n",
    "        \n",
    "original_audio_path = \"audio_input.mp3\"  # Path to the original audio file\n",
    "output_dir = \"speaker_audio\"  # Directory to save the output files\n",
    "split_audio_by_speaker(regrouped_segments, original_audio_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_0\n",
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_1\n",
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_2\n",
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_3\n",
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_4\n",
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_5\n",
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_6\n",
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_7\n",
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_8\n",
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_9\n",
      "Voice conversion completed. Output directory: c:\\Users\\mimo6\\Autodubbing\\autodubbing\\vc_segments\\segment_10\n",
      "Voice conversion completed for 11 segments.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Function to calculate audio duration in seconds\n",
    "def get_audio_duration(file_path):\n",
    "    audio = AudioSegment.from_file(file_path)\n",
    "    return len(audio) / 1000.0  # Duration in seconds\n",
    "\n",
    "# Function to determine length adjustment factor\n",
    "def calculate_length_adjust(source_duration, reference_duration):\n",
    "    ratio = reference_duration / source_duration\n",
    "    # Apply bounds\n",
    "    if ratio > 1.25:\n",
    "        return 1.25\n",
    "    elif ratio < 0.75:\n",
    "        return 0.75\n",
    "    return ratio\n",
    "\n",
    "# Function to run SEED-VC for voice conversion\n",
    "def run_seed_vc(source, target, output_dir, length_adjust):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    command = [\n",
    "        \".venv\\\\Scripts\\\\python\", \"seed-vc-main/inference.py\",\n",
    "        \"--source\", source,\n",
    "        \"--target\", target,\n",
    "        \"--output\", output_dir,\n",
    "        \"--diffusion-steps\", \"25\",\n",
    "        \"--length-adjust\", str(length_adjust),\n",
    "        \"--inference-cfg-rate\", \"0.7\"\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True, encoding=\"utf-8\")\n",
    "        print(f\"Voice conversion completed. Output directory: {output_dir}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during voice conversion for {source} and {target}:\")\n",
    "        print(e.stderr)\n",
    "        raise e\n",
    "\n",
    "# Paths and directories\n",
    "temp_dir = os.path.abspath(\"tempfiles\")  # Use absolute path for temp files\n",
    "output_base_dir = os.path.abspath(\"vc_segments\")  # Base directory for SEED-VC outputs\n",
    "speaker_audio_dir = os.path.abspath(\"speaker_audio\")  # Directory for speaker audio files\n",
    "\n",
    "# Dynamically map speaker audio files\n",
    "speaker_to_file = {}\n",
    "\n",
    "# List all files in the speaker_audio directory\n",
    "for file_name in os.listdir(speaker_audio_dir):\n",
    "    if file_name.endswith(\".wav\"):  # Ensure it's a WAV file\n",
    "        # Extract the speaker name from the file name\n",
    "        speaker_name = file_name.replace(\".wav\", \"\").replace(\"_\", \" \").title()\n",
    "        # Map the speaker to the file path\n",
    "        speaker_to_file[speaker_name] = os.path.join(speaker_audio_dir, file_name)\n",
    "\n",
    "# Iterate over all segments\n",
    "converted_segments = []  # To store paths of converted audio segments\n",
    "for i, segment in enumerate(regrouped_segments):\n",
    "    speaker = segment[\"speaker\"]\n",
    "    segment = segment[\"text\"]\n",
    "\n",
    "    # Files for this segment\n",
    "    source_file = os.path.join(temp_dir, f\"translated_segment{i}.wav\")\n",
    "    reference_length_file = os.path.join(temp_dir, f\"segment{i}.wav\")  # Used to calculate length_adjust\n",
    "    target_speaker_file = speaker_to_file[speaker]  # Speaker-specific reference file for conversion\n",
    "    output_dir = os.path.join(output_base_dir, f\"segment_{i}\")  # Unique directory for each segment\n",
    "\n",
    "    # Check if files exist\n",
    "    if not os.path.exists(source_file) or not os.path.exists(reference_length_file) or not os.path.exists(target_speaker_file):\n",
    "        print(f\"Skipping segment {i}: Missing file(s).\")\n",
    "        continue\n",
    "\n",
    "    # Calculate durations\n",
    "    source_duration = get_audio_duration(source_file)\n",
    "    reference_duration = get_audio_duration(reference_length_file)\n",
    "\n",
    "    # Determine length adjustment\n",
    "    length_adjust = calculate_length_adjust(source_duration, reference_duration)\n",
    "\n",
    "    # Run voice conversion and get the output file\n",
    "    converted_file = run_seed_vc(source_file, target_speaker_file, output_dir, length_adjust)\n",
    "    converted_segments.append(converted_file)\n",
    "\n",
    "# Summary\n",
    "print(f\"Voice conversion completed for {len(converted_segments)} segments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final audio saved as output_with_timing.mp3\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Example data\n",
    "# `translated_segments` contains start and end times (in seconds) for each segment\n",
    "# Path to the vc_segments directory\n",
    "vc_segments_dir = \"vc_segments\"\n",
    "\n",
    "# Step 1: Calculate the total duration and create silent audio\n",
    "max_duration = max(int(data[\"end\"] * 1000) for data in translated_segments)  # In milliseconds\n",
    "final_audio = AudioSegment.silent(duration=max_duration)\n",
    "\n",
    "# Step 2: Collect all processed audio files from the vc_segments directory\n",
    "audio_files = []\n",
    "for i in range(len(translated_segments)):\n",
    "    segment_dir = os.path.join(vc_segments_dir, f\"segment_{i}\")\n",
    "    # Select any file in the directory (assuming there is only one file per directory)\n",
    "    segment_files = glob.glob(os.path.join(segment_dir, \"*\"))  # Matches all files in the directory\n",
    "    if segment_files:\n",
    "        audio_files.append(segment_files[0])  # Take the first file found\n",
    "    else:\n",
    "        print(f\"No file found in {segment_dir}. Skipping...\")\n",
    "\n",
    "# Step 3: Overlay each segment at its corresponding start time\n",
    "for data, file_path in zip(translated_segments, audio_files):\n",
    "    start_time = int(data[\"start\"] * 1000)  # Convert start time to milliseconds\n",
    "\n",
    "    # Load the audio file\n",
    "    segment = AudioSegment.from_file(file_path)\n",
    "\n",
    "    # Overlay the segment at the specified start time\n",
    "    final_audio = final_audio.overlay(segment, position=start_time)\n",
    "\n",
    "# Step 4: Export the final audio\n",
    "final_audio.export(\"output_with_timing.mp3\", format=\"mp3\")\n",
    "print(\"Final audio saved as output_with_timing.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video with new audio saved at: video_output.mp4\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Paths to your video and audio files\n",
    "video_path = \"video_input.mp4\"\n",
    "audio_path = \"output_with_timing.mp3\"\n",
    "output_path = \"video_output.mp4\"\n",
    "\n",
    "\n",
    "\n",
    "# FFmpeg command to replace the audio\n",
    "command = [\n",
    "    \"ffmpeg\",\n",
    "    \"-y\",                   # Overwrite output without asking\n",
    "    \"-i\", video_path,       # Input video\n",
    "    \"-i\", audio_path,       # Input audio\n",
    "    \"-c:v\", \"copy\",         # Copy video codec (no re-encoding)\n",
    "    \"-c:a\", \"aac\",          # Encode audio with AAC codec\n",
    "    \"-map\", \"0:v:0\",        # Use video from the first input\n",
    "    \"-map\", \"1:a:0\",        # Use audio from the second input\n",
    "    \"-shortest\",            # Match the duration of the shorter input\n",
    "    output_path             # Output file\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command, check=True)\n",
    "\n",
    "print(f\"Video with new audio saved at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoDubbing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
